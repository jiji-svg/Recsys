gradient_accumulation_steps: 2
logging_steps: 4
save_strategy: "epoch"
optim: "paged_adamw_32bit"
bf16: True
fp16: False
tf32: True
max_grad_norm: 1.0
warmup_ratio: 0.06
lr_scheduler_type: "cosine"
disable_tqdm: False
weight_decay: 0.01
num_epochs: 3
batch_size: 4
learning_rate: 4e-4
output_dir: "checkpoints"
push_to_hub: False
save_path: "./finetuned_model"
lora_model_path: "lora_adapter"
hub_model_id: "whbye-choi/OPEN-SOLAR-KO-10.7B-exp"